# An√°lisis: Scrapers Experimentales vs Producci√≥n

**Fecha:** 2026-01-27  
**Objetivo:** Analizar los scrapers de producci√≥n en `legacy/src/` para identificar qu√© est√° bien, qu√© est√° mal, y qu√© mejoras aplicar al convertir los experimentales a producci√≥n.

---

## Resumen Ejecutivo

Los scrapers de producci√≥n (`legacy/src/`) tienen una arquitectura mucho m√°s robusta que los experimentales, pero tambi√©n tienen complejidad innecesaria y algunos problemas de dise√±o. Este an√°lisis identifica los puntos clave para crear una versi√≥n mejorada.

---

## 1. Scraper Regional

### Archivos Analizados
- **Experimental:** `experiments/por_region/scraper_regional.py` (289 l√≠neas)
- **Producci√≥n:** `legacy/src/regional/scraper.py` (1275 l√≠neas)
- **Config:** `legacy/src/regional/config.py` (189 l√≠neas)
- **Selectors:** `legacy/src/regional/selectors.py` (59 l√≠neas)

### ‚úÖ Lo que est√° BIEN en Producci√≥n

#### 1.1 Arquitectura Modular
- **Separaci√≥n de responsabilidades:** Config, selectors y scraper est√°n separados
- **Selectores centralizados:** Facilita mantenimiento cuando cambia la estructura del sitio
- **Configuraci√≥n externa:** YAML + variables de entorno, muy flexible

#### 1.2 Manejo de Errores Robusto
- **Excepciones personalizadas:** `ElementNotFoundError`, `TableNotFoundError`, `InvalidTableStructureError`
- **Logging profesional:** Sistema de logging con niveles configurables
- **Validaci√≥n de estructura:** Verifica que las tablas tengan el formato esperado antes de extraer

#### 1.3 Funcionalidades Avanzadas
- **Context manager async:** `async with SeaceScraper() as scraper:` - manejo seguro de recursos
- **Guardado incremental:** Sistema de checkpoints para reanudar scraping
- **Monitoreo de red:** En modo debug, captura peticiones AJAX para debugging
- **Espera inteligente:** Detecta respuestas AJAX de JSF antes de validar tabla

#### 1.4 Validaci√≥n de Datos
- **Validaci√≥n de estructura de tabla:** Verifica n√∫mero de columnas antes de extraer
- **Manejo de casos edge:** Detecta "no hay resultados" vs "tabla vac√≠a por error"
- **Retry con backoff exponencial:** En `_wait_for_table_ready()`

### ‚ùå Lo que est√° MAL en Producci√≥n

#### 1.1 Complejidad Excesiva
- **1275 l√≠neas es demasiado:** Mucho c√≥digo para debugging que no deber√≠a estar en producci√≥n
- **Monitoreo de red innecesario:** `_enable_network_monitoring()`, `_capture_network_snapshot()`, `_analyze_network_changes()` - esto deber√≠a ser opcional o estar en herramientas separadas
- **M√∫ltiples m√©todos de espera:** `_wait_for_loading_indicators_to_disappear()`, `_wait_for_jsf_ajax_response()`, `_wait_for_table_ready()` - demasiada complejidad

#### 1.2 Problemas de Dise√±o
- **Dependencia de YAML:** Requiere `pyyaml` como dependencia adicional
- **Configuraci√≥n duplicada:** `RegionalConfig` y `NomenclaturaConfig` tienen mucho c√≥digo duplicado
- **Selectores hardcodeados:** Aunque est√°n centralizados, algunos selectores son muy espec√≠ficos y fr√°giles

#### 1.3 C√≥digo Problem√°tico
- **Delays fijos:** `await asyncio.sleep(2)` en varios lugares - deber√≠a ser configurable
- **Manejo de errores inconsistente:** A veces contin√∫a, a veces lanza excepci√≥n
- **Validaci√≥n redundante:** Valida estructura de tabla m√∫ltiples veces

#### 1.4 Problemas de Performance
- **Espera excesiva:** `await asyncio.sleep(7)` despu√©s de buscar - demasiado tiempo
- **M√∫ltiples esperas:** Espera networkidle + sleep + validaci√≥n - podr√≠a optimizarse
- **Guardado incremental puede ser lento:** Escribe CSV despu√©s de cada p√°gina

### üîß Mejoras Sugeridas

#### 1.1 Simplificar Esperas
- **Una sola funci√≥n de espera inteligente:** Combinar las m√∫ltiples funciones en una que detecte autom√°ticamente cuando la p√°gina est√° lista
- **Usar eventos de Playwright:** En lugar de esperas fijas, usar `page.wait_for_response()` o `page.wait_for_selector()`
- **Timeout configurable:** Pero con valores razonables por defecto

#### 1.2 Reducir Complejidad
- **Eliminar monitoreo de red:** Moverlo a herramientas de debugging separadas
- **Simplificar validaci√≥n:** Una sola validaci√≥n antes de extraer, no m√∫ltiples
- **C√≥digo m√°s limpio:** Reducir de 1275 a ~600-700 l√≠neas manteniendo funcionalidad

#### 1.3 Mejorar Configuraci√≥n
- **Configuraci√≥n unificada:** Una sola clase base `BaseConfig` con herencia
- **Sin YAML obligatorio:** Usar valores por defecto sensatos, YAML opcional
- **Type hints mejorados:** Usar `TypedDict` o `pydantic` para validaci√≥n de config

#### 1.4 Optimizar Performance
- **Esperas m√°s inteligentes:** Detectar cuando la tabla est√° lista en lugar de esperas fijas
- **Guardado en batch:** Guardar cada N p√°ginas en lugar de cada p√°gina
- **Paralelizaci√≥n opcional:** Para m√∫ltiples departamentos/a√±os

---

## 2. Scraper por Nomenclatura

### Archivos Analizados
- **Experimental:** `experiments/por_nomenclatura/scraper_nomenclatura.py` (344 l√≠neas)
- **Producci√≥n:** `legacy/src/nomenclatura/scraper.py` (629 l√≠neas)
- **Config:** `legacy/src/nomenclatura/config.py` (173 l√≠neas)
- **Selectors:** `legacy/src/nomenclatura/selectors.py` (63 l√≠neas)

### ‚úÖ Lo que est√° BIEN en Producci√≥n

#### 2.1 Extracci√≥n de Datos
- **Extracci√≥n de cronograma:** Bien estructurada con validaci√≥n de columnas
- **Extracci√≥n de documentos:** Maneja links de descarga correctamente
- **Manejo de errores por fila:** Si una fila falla, contin√∫a con las dem√°s

#### 2.2 Validaci√≥n
- **Validaci√≥n de estructura:** Verifica n√∫mero m√≠nimo de celdas antes de extraer
- **Manejo de casos edge:** Detecta cuando no hay suficientes celdas

#### 2.3 Logging
- **Logging detallado:** Informa progreso de extracci√≥n
- **Debug opcional:** Guarda HTMLs cuando est√° en modo debug

### ‚ùå Lo que est√° MAL en Producci√≥n

#### 2.1 Problemas con Links de Descarga
- **M√©todo complejo:** Intenta interceptar descarga, luego interceptar respuesta de red - demasiado complejo
- **C√≥digo duplicado:** El m√©todo experimental tiene mejor l√≥gica para obtener links
- **No maneja bien errores:** Si falla obtener link, contin√∫a pero no informa bien

#### 2.2 Esperas Excesivas
- **`await asyncio.sleep(7)` despu√©s de buscar:** Demasiado tiempo fijo
- **`await asyncio.sleep(3)` despu√©s de clickear ficha:** Podr√≠a ser m√°s inteligente
- **Delays entre documentos:** Configurable pero valor por defecto puede ser optimizado

#### 2.3 C√≥digo Experimental Mejor en Algunos Aspectos
- **M√©todo de obtener links:** El experimental tiene mejor l√≥gica con `page.expect_download()`
- **Manejo de errores m√°s simple:** El experimental es m√°s directo

### üîß Mejoras Sugeridas

#### 2.1 Simplificar Obtenci√≥n de Links
- **Usar m√©todo del experimental:** `page.expect_download()` es m√°s simple y confiable
- **Fallback simple:** Si falla, intentar m√©todo alternativo una vez, luego continuar
- **No interceptar todas las respuestas:** Solo interceptar cuando sea necesario

#### 2.2 Optimizar Esperas
- **Esperar a que la tabla est√© visible:** En lugar de sleep fijo
- **Detectar cuando carg√≥:** Usar `wait_for_selector()` con estado visible
- **Reducir delays:** Valores por defecto m√°s agresivos pero seguros

#### 2.3 Mejorar Extracci√≥n
- **Validar estructura antes de iterar:** Una sola validaci√≥n al inicio
- **Manejo de errores m√°s claro:** Informar mejor qu√© fall√≥ y por qu√©

---

## 3. Comparaci√≥n: Experimental vs Producci√≥n

### Aspectos donde el EXPERIMENTAL es mejor

1. **Simplicidad:** El c√≥digo experimental es m√°s f√°cil de entender
2. **Obtenci√≥n de links:** El m√©todo con `expect_download()` es m√°s directo
3. **Menos dependencias:** No requiere YAML, menos complejidad
4. **C√≥digo m√°s directo:** Menos abstracciones innecesarias

### Aspectos donde la PRODUCCI√ìN es mejor

1. **Arquitectura modular:** Separaci√≥n clara de responsabilidades
2. **Manejo de errores:** Excepciones personalizadas y logging profesional
3. **Configuraci√≥n flexible:** YAML + variables de entorno
4. **Validaci√≥n robusta:** Verifica estructura antes de extraer
5. **Context manager:** Manejo seguro de recursos
6. **Guardado incremental:** Sistema de checkpoints (solo en regional)

---

## 4. Recomendaciones para Nueva Versi√≥n de Producci√≥n

### 4.1 Arquitectura

```
src/
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # Clase base con funcionalidad com√∫n
‚îÇ   ‚îú‚îÄ‚îÄ regional.py          # Scraper regional (simplificado)
‚îÇ   ‚îî‚îÄ‚îÄ nomenclatura.py      # Scraper nomenclatura (simplificado)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # Configuraci√≥n base (sin YAML obligatorio)
‚îÇ   ‚îî‚îÄ‚îÄ settings.py          # Configuraci√≥n compartida
‚îú‚îÄ‚îÄ selectors/
‚îÇ   ‚îú‚îÄ‚îÄ regional.py          # Selectores regionales
‚îÇ   ‚îî‚îÄ‚îÄ nomenclatura.py      # Selectores nomenclatura
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ exceptions.py        # Excepciones personalizadas
    ‚îî‚îÄ‚îÄ logging.py           # Utilidades de logging
```

### 4.2 Principios de Dise√±o

1. **Simplicidad sobre complejidad:** Menos c√≥digo, m√°s mantenible
2. **Configuraci√≥n opcional:** Valores por defecto sensatos, configuraci√≥n externa opcional
3. **Una responsabilidad:** Cada funci√≥n hace una cosa bien
4. **Esperas inteligentes:** Detectar estado en lugar de esperas fijas
5. **Manejo de errores claro:** Excepciones espec√≠ficas, logging √∫til

### 4.3 Caracter√≠sticas a Mantener

- ‚úÖ Context manager async
- ‚úÖ Logging profesional
- ‚úÖ Excepciones personalizadas
- ‚úÖ Validaci√≥n de estructura
- ‚úÖ Selectores centralizados
- ‚úÖ Configuraci√≥n flexible (pero opcional)

### 4.4 Caracter√≠sticas a Eliminar/Simplificar

- ‚ùå Monitoreo de red complejo (mover a herramientas de debug)
- ‚ùå M√∫ltiples m√©todos de espera (unificar en uno inteligente)
- ‚ùå Dependencia de YAML (hacer opcional)
- ‚ùå Guardado incremental complejo (simplificar o hacer opcional)
- ‚ùå Delays fijos excesivos (usar esperas inteligentes)

### 4.5 Caracter√≠sticas a Mejorar

- üîß Obtenci√≥n de links: Usar m√©todo del experimental mejorado
- üîß Esperas: Combinar m√∫ltiples m√©todos en uno inteligente
- üîß Configuraci√≥n: Clase base con herencia, sin duplicaci√≥n
- üîß Validaci√≥n: Una sola validaci√≥n eficiente
- üîß Performance: Reducir esperas innecesarias

---

## 5. Plan de Implementaci√≥n

### Fase 1: Base Com√∫n
1. Crear `src/scrapers/base.py` con funcionalidad compartida
2. Crear `src/config/settings.py` con configuraci√≥n unificada
3. Crear `src/utils/exceptions.py` y `src/utils/logging.py`

### Fase 2: Scraper Regional
1. Crear `src/scrapers/regional.py` basado en experimental pero con mejoras de producci√≥n
2. Crear `src/selectors/regional.py` con selectores centralizados
3. Implementar validaci√≥n simplificada pero robusta

### Fase 3: Scraper Nomenclatura
1. Crear `src/scrapers/nomenclatura.py` mejorando ambos (experimental + producci√≥n)
2. Crear `src/selectors/nomenclatura.py` con selectores centralizados
3. Implementar obtenci√≥n de links mejorada

### Fase 4: Optimizaci√≥n
1. Reducir complejidad innecesaria
2. Optimizar esperas y timeouts
3. Mejorar manejo de errores
4. Agregar tests

---

## 6. Checklist de Calidad

Para cada scraper de producci√≥n, verificar:

- [ ] Menos de 800 l√≠neas de c√≥digo
- [ ] Sin dependencias innecesarias (YAML opcional)
- [ ] Esperas inteligentes, no fijas
- [ ] Una sola funci√≥n de validaci√≥n eficiente
- [ ] Context manager async implementado
- [ ] Logging profesional configurado
- [ ] Excepciones personalizadas apropiadas
- [ ] Selectores centralizados y documentados
- [ ] Configuraci√≥n con valores por defecto sensatos
- [ ] Manejo de errores claro y consistente
- [ ] Sin c√≥digo de debugging en producci√≥n
- [ ] Type hints completos
- [ ] Docstrings claros y completos

---

## Conclusi√≥n

Los scrapers de producci√≥n tienen una base s√≥lida pero est√°n sobrecargados con complejidad innecesaria. La nueva versi√≥n debe:

1. **Mantener lo bueno:** Arquitectura modular, manejo de errores, logging
2. **Eliminar lo malo:** Complejidad excesiva, dependencias innecesarias, esperas fijas
3. **Mejorar lo existente:** Simplificar esperas, optimizar performance, mejorar obtenci√≥n de links
4. **Aprender del experimental:** Simplicidad, c√≥digo directo, menos abstracciones

El objetivo es crear scrapers que sean **robustos pero simples**, **flexibles pero con buenos defaults**, y **f√°ciles de mantener y extender**.

